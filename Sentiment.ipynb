{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4Tb_J1uOkFT",
        "colab_type": "code",
        "outputId": "741b5815-3a3c-4537-aea3-73f37e3c611e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount = True)\n",
        "dataset_path = 'gdrive/My Drive/Projects/Sentiment Analysis/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwqFdJlFUkcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_excel(dataset_path + \"train.xlsx\", encoding = 'latin-1')\n",
        "test_df = pd.read_excel(dataset_path + \"test.xlsx\", encoding = 'latin-1')\n",
        "X_train, y_train = train_df['Review'], train_df['Sentiment']\n",
        "X_test, y_test = test_df['Review'], test_df['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kETuUmdkezpw",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n",
        "\n",
        "\n",
        "\n",
        "1.   All punctuation marks are removed from the data and replaced by \" \"\n",
        "2.   All special characters are removed from both the train and test reviews\n",
        "3. All words are converted to their lower case versions\n",
        "4. Stopwords are removed from the reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6qe3qM_-c6Y",
        "colab_type": "code",
        "outputId": "3baf764f-01d1-48e2-a390-3b83c461e6eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import string\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lmtzr = WordNetLemmatizer()\n",
        "stop_words = stopwords.words('english')\n",
        "X_train_mod, y_train_mod = [], []\n",
        "X_test_mod, y_test_mod = [], []\n",
        "words = []\n",
        "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "for ind, review in enumerate(X_train):\n",
        "  try:\n",
        "    word_review = review.split(' ')\n",
        "    word_review = [w.translate(table) for w in word_review]\n",
        "    word_review = [w for w in word_review if w.isalpha()]\n",
        "    word_review = [w.lower() for w in word_review if w not in stop_words]\n",
        "    word_review = [lmtzr.lemmatize(w) for w in word_review]\n",
        "    X_train_mod.append(' '.join(word_review))\n",
        "    y_train_mod.append(y_train[ind])\n",
        "    words += word_review\n",
        "  except:\n",
        "    continue\n",
        "for ind, review in enumerate(X_test):\n",
        "  try:\n",
        "    word_review = review.split(' ')\n",
        "    word_review = [w.translate(table) for w in word_review]\n",
        "    word_review = [w for w in word_review if w.isalpha()]\n",
        "    word_review = [w.lower() for w in word_review if w not in stop_words]\n",
        "    word_review = [lmtzr.lemmatize(w) for w in word_review]\n",
        "    X_test_mod.append(' '.join(word_review))\n",
        "    y_test_mod.append(y_test[ind])\n",
        "    words += word_review\n",
        "  except:\n",
        "    continue\n",
        "    \n",
        "X_total = X_train_mod + X_test_mod"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFR3YDGlfrqD",
        "colab_type": "text"
      },
      "source": [
        "# Embedding \n",
        "\n",
        "The Glove vector embedding that has been downloaded is used to create an embedding vector for each word and stored in the dictionary **embedding**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mx9aGjWpFoHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding = {}\n",
        "f = open('gdrive/My Drive/glove.840B.300d.txt')\n",
        "\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    try:\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embedding[word] = coefs\n",
        "    except:\n",
        "      continue\n",
        "    \n",
        "f.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXZu6KaFgMIP",
        "colab_type": "text"
      },
      "source": [
        "Keras is used to create sequences for all the data and the maximum length of reviews across both train and test data sets is used to pad the sequences\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ib5FnNrQ6ZNM",
        "colab_type": "code",
        "outputId": "ab092138-efc8-43dd-d1c4-f846cb57ae94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_len = max(len(l.split()) for l in X_total)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_total)\n",
        "sequences = tokenizer.texts_to_sequences(X_train_mod)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "\n",
        "review_pad = pad_sequences(sequences, maxlen = max_len)\n",
        "print(review_pad.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "72959\n",
            "(24999, 1225)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZcBMzUP7ewM",
        "colab_type": "code",
        "outputId": "fcca386e-3f65-4756-ad71-18bb3922cd2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train_mod = np.asarray(y_train_mod).reshape(len(y_train_mod), 1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(24999, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j88ZpziugmE6",
        "colab_type": "text"
      },
      "source": [
        "An embedding matrix is created for the words in the sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RLelYuuFXCl",
        "colab_type": "code",
        "outputId": "8e7c409f-e5d1-4509-c6ae-d58db66c8cd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "num_words = len(word_index) + 1\n",
        "embedding_matrix = np.zeros((num_words, 300))\n",
        "for word, i in word_index.items():\n",
        "  if i > num_words:\n",
        "    continue\n",
        "  embedding_vector = embedding.get(word)\n",
        "  if embedding_vector is not None:\n",
        "    embedding_matrix[i] = embedding_vector\n",
        "    \n",
        "print(num_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72960\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdk7SO70gzQq",
        "colab_type": "text"
      },
      "source": [
        "# Model\n",
        "\n",
        "\n",
        "*   A **Bidirectional GRU of 64 layers with dropouts of 0.2** is followed by a **Dense layer of 1** unit\n",
        "*  **Sigmoid activation** is used in the last layer and the loss is **binary cross-entropy**\n",
        "* As we are using pretraine GLoVE model we have kept the embedding layer as untrainable\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0lY7PX5GWHt",
        "colab_type": "code",
        "outputId": "54ea1991-03c3-4606-e6d1-bbf9b31f7802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        }
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, GRU, Bidirectional\n",
        "from keras.initializers import Constant\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "embedding_layer = Embedding(num_words,\n",
        "                           300,\n",
        "                           embeddings_initializer = Constant(embedding_matrix),\n",
        "                           input_length = max_len,\n",
        "                           trainable=False)\n",
        "model.add(embedding_layer)\n",
        "model.add(Bidirectional(GRU(units=64, dropout=0.2, recurrent_dropout = 0.2)))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0623 04:10:04.974187 140687264831360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0623 04:10:05.012596 140687264831360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0623 04:10:05.579812 140687264831360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0623 04:10:05.769470 140687264831360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0623 04:10:05.781176 140687264831360 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0623 04:10:06.220898 140687264831360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0623 04:10:06.243581 140687264831360 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0623 04:10:06.249322 140687264831360 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f5XinkUhmsZ",
        "colab_type": "text"
      },
      "source": [
        "Validation data is 20% of the train data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzo1oc3iHzVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_split = 0.2\n",
        "indices = np.arange(review_pad.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "review_pad = review_pad[indices]\n",
        "sentiment = y_train_mod[indices]\n",
        "num_validation_samples = int(val_split*review_pad.shape[0])\n",
        "\n",
        "X_train_pad = review_pad[:-num_validation_samples]\n",
        "y_train_pad = sentiment[:-num_validation_samples]\n",
        "X_test_pad = review_pad[-num_validation_samples:]\n",
        "y_test_pad = sentiment[-num_validation_samples:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFTQ8xLJ8Evi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "checkpoint_path = \"cp-{epoch:04d}.ckpt\"\n",
        "\n",
        "cp_callback = ModelCheckpoint(checkpoint_path, verbose=1,\n",
        "                              save_weights_only=True,\n",
        "                              period=5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCpkmkhZI5GD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(X_train_pad, y_train_pad, batch_size = 128, epochs = 25,\n",
        "          validation_data = (X_test_pad, y_test_pad),\n",
        "          callbacks = [cp_callback], verbose = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeiKmdeuJNJq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('trained_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b71MBFsjwwh3",
        "colab_type": "code",
        "outputId": "d34eb6de-101b-4161-97b1-d50a41b351ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "sequences = tokenizer.texts_to_sequences(X_test_mod)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "\n",
        "test_pad = pad_sequences(sequences, maxlen = max_len)\n",
        "print(test_pad.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "72959\n",
            "(24998, 1225)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq7glotFjpz6",
        "colab_type": "code",
        "outputId": "c88251ba-0d08-40c1-8500-89616b201cce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "model = load_model('trained_model.h5')\n",
        "loss, acc = model.evaluate(test_pad, np.asarray(y_test_mod))\n",
        "print(\"Restored model, accuracy: {:5.2f}%\".format(100*acc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "24998/24998 [==============================] - 1005s 40ms/step\n",
            "Restored model, accuracy: 86.70%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}